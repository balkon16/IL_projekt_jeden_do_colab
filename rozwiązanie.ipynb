{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/pawel/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/pawel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Załadowanie plików z danymi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pliki z danymi znajduję się w folderze `./data/`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.tsv  train.tsv  valid.tsv\n"
     ]
    }
   ],
   "source": [
    "!ls ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./data/train.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W danych walidacyjnych i testowych brakuje nagłówków, więc dodaję je z pliku treningowego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = pd.read_csv('./data/valid.tsv', sep='\\t', header=None, names=train_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140461</td>\n",
       "      <td>7622</td>\n",
       "      <td>democracy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>140462</td>\n",
       "      <td>7622</td>\n",
       "      <td>and civic action laudable</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>140463</td>\n",
       "      <td>7622</td>\n",
       "      <td>civic action laudable</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>140464</td>\n",
       "      <td>7622</td>\n",
       "      <td>action laudable</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>140465</td>\n",
       "      <td>7623</td>\n",
       "      <td>Griffin &amp; Co. manage to be spectacularly outra...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0    140461        7622                                          democracy   \n",
       "1    140462        7622                          and civic action laudable   \n",
       "2    140463        7622                              civic action laudable   \n",
       "3    140464        7622                                    action laudable   \n",
       "4    140465        7623  Griffin & Co. manage to be spectacularly outra...   \n",
       "\n",
       "   Sentiment  \n",
       "0          2  \n",
       "1          3  \n",
       "2          2  \n",
       "3          3  \n",
       "4          4  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7795</th>\n",
       "      <td>148256</td>\n",
       "      <td>8068</td>\n",
       "      <td>a storm</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7796</th>\n",
       "      <td>148257</td>\n",
       "      <td>8068</td>\n",
       "      <td>as a fringe feminist conspiracy theorist</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7797</th>\n",
       "      <td>148258</td>\n",
       "      <td>8068</td>\n",
       "      <td>a fringe feminist conspiracy theorist</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7798</th>\n",
       "      <td>148259</td>\n",
       "      <td>8068</td>\n",
       "      <td>fringe feminist conspiracy theorist</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7799</th>\n",
       "      <td>148260</td>\n",
       "      <td>8068</td>\n",
       "      <td>fringe</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      PhraseId  SentenceId                                    Phrase  \\\n",
       "7795    148256        8068                                   a storm   \n",
       "7796    148257        8068  as a fringe feminist conspiracy theorist   \n",
       "7797    148258        8068     a fringe feminist conspiracy theorist   \n",
       "7798    148259        8068       fringe feminist conspiracy theorist   \n",
       "7799    148260        8068                                    fringe   \n",
       "\n",
       "      Sentiment  \n",
       "7795          2  \n",
       "7796          1  \n",
       "7797          2  \n",
       "7798          1  \n",
       "7799          2  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('./data/test.tsv', sep='\\t', header=None, names=train_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148261</td>\n",
       "      <td>8068</td>\n",
       "      <td>feminist conspiracy theorist</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>148262</td>\n",
       "      <td>8068</td>\n",
       "      <td>conspiracy theorist</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>148263</td>\n",
       "      <td>8068</td>\n",
       "      <td>theorist</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>148264</td>\n",
       "      <td>8068</td>\n",
       "      <td>named Dirty Dick</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>148265</td>\n",
       "      <td>8068</td>\n",
       "      <td>Dirty Dick</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                        Phrase  Sentiment\n",
       "0    148261        8068  feminist conspiracy theorist          2\n",
       "1    148262        8068           conspiracy theorist          2\n",
       "2    148263        8068                      theorist          2\n",
       "3    148264        8068              named Dirty Dick          2\n",
       "4    148265        8068                    Dirty Dick          2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7795</th>\n",
       "      <td>156056</td>\n",
       "      <td>8544</td>\n",
       "      <td>Hearst 's</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7796</th>\n",
       "      <td>156057</td>\n",
       "      <td>8544</td>\n",
       "      <td>forced avuncular chortles</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7797</th>\n",
       "      <td>156058</td>\n",
       "      <td>8544</td>\n",
       "      <td>avuncular chortles</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7798</th>\n",
       "      <td>156059</td>\n",
       "      <td>8544</td>\n",
       "      <td>avuncular</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7799</th>\n",
       "      <td>156060</td>\n",
       "      <td>8544</td>\n",
       "      <td>chortles</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      PhraseId  SentenceId                     Phrase  Sentiment\n",
       "7795    156056        8544                  Hearst 's          2\n",
       "7796    156057        8544  forced avuncular chortles          1\n",
       "7797    156058        8544         avuncular chortles          3\n",
       "7798    156059        8544                  avuncular          2\n",
       "7799    156060        8544                   chortles          2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdzam czy istnieją komórki z wartością `NaN`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False, False]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: x.isnull().any().any(), (train_data, valid_data, test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wstępna analiza danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forma i wygląd danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zdania oznaczone przez `SentenceId` są podzielone na frazy (`Phrase`), z których każda posiada określony `Sentiment`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1134</th>\n",
       "      <td>1135</td>\n",
       "      <td>42</td>\n",
       "      <td>Vincent Gallo is right at home in this French ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>1136</td>\n",
       "      <td>42</td>\n",
       "      <td>Vincent Gallo</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136</th>\n",
       "      <td>1137</td>\n",
       "      <td>42</td>\n",
       "      <td>Vincent</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>1138</td>\n",
       "      <td>42</td>\n",
       "      <td>Gallo</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>1139</td>\n",
       "      <td>42</td>\n",
       "      <td>is right at home in this French shocker playin...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>1140</td>\n",
       "      <td>42</td>\n",
       "      <td>is right at home in this French shocker playin...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140</th>\n",
       "      <td>1141</td>\n",
       "      <td>42</td>\n",
       "      <td>is right at home in this French shocker</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1141</th>\n",
       "      <td>1142</td>\n",
       "      <td>42</td>\n",
       "      <td>right at home in this French shocker</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>1143</td>\n",
       "      <td>42</td>\n",
       "      <td>right at home</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143</th>\n",
       "      <td>1144</td>\n",
       "      <td>42</td>\n",
       "      <td>right</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <td>1145</td>\n",
       "      <td>42</td>\n",
       "      <td>at home</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1145</th>\n",
       "      <td>1146</td>\n",
       "      <td>42</td>\n",
       "      <td>at</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1146</th>\n",
       "      <td>1147</td>\n",
       "      <td>42</td>\n",
       "      <td>home</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>1148</td>\n",
       "      <td>42</td>\n",
       "      <td>in this French shocker</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>1149</td>\n",
       "      <td>42</td>\n",
       "      <td>this French shocker</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149</th>\n",
       "      <td>1150</td>\n",
       "      <td>42</td>\n",
       "      <td>French shocker</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>1151</td>\n",
       "      <td>42</td>\n",
       "      <td>French</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>1152</td>\n",
       "      <td>42</td>\n",
       "      <td>shocker</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152</th>\n",
       "      <td>1153</td>\n",
       "      <td>42</td>\n",
       "      <td>playing his usual bad boy weirdo role</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1153</th>\n",
       "      <td>1154</td>\n",
       "      <td>42</td>\n",
       "      <td>playing</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>1155</td>\n",
       "      <td>42</td>\n",
       "      <td>his usual bad boy weirdo role</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>1156</td>\n",
       "      <td>42</td>\n",
       "      <td>usual bad boy weirdo role</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>1157</td>\n",
       "      <td>42</td>\n",
       "      <td>usual</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <td>1158</td>\n",
       "      <td>42</td>\n",
       "      <td>bad boy weirdo role</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1158</th>\n",
       "      <td>1159</td>\n",
       "      <td>42</td>\n",
       "      <td>bad</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>1160</td>\n",
       "      <td>42</td>\n",
       "      <td>boy weirdo role</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>1161</td>\n",
       "      <td>42</td>\n",
       "      <td>boy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>1162</td>\n",
       "      <td>42</td>\n",
       "      <td>weirdo role</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1162</th>\n",
       "      <td>1163</td>\n",
       "      <td>42</td>\n",
       "      <td>weirdo</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>1164</td>\n",
       "      <td>42</td>\n",
       "      <td>role</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      PhraseId  SentenceId                                             Phrase  \\\n",
       "1134      1135          42  Vincent Gallo is right at home in this French ...   \n",
       "1135      1136          42                                      Vincent Gallo   \n",
       "1136      1137          42                                            Vincent   \n",
       "1137      1138          42                                              Gallo   \n",
       "1138      1139          42  is right at home in this French shocker playin...   \n",
       "1139      1140          42  is right at home in this French shocker playin...   \n",
       "1140      1141          42            is right at home in this French shocker   \n",
       "1141      1142          42               right at home in this French shocker   \n",
       "1142      1143          42                                      right at home   \n",
       "1143      1144          42                                              right   \n",
       "1144      1145          42                                            at home   \n",
       "1145      1146          42                                                 at   \n",
       "1146      1147          42                                               home   \n",
       "1147      1148          42                             in this French shocker   \n",
       "1148      1149          42                                this French shocker   \n",
       "1149      1150          42                                     French shocker   \n",
       "1150      1151          42                                             French   \n",
       "1151      1152          42                                            shocker   \n",
       "1152      1153          42              playing his usual bad boy weirdo role   \n",
       "1153      1154          42                                            playing   \n",
       "1154      1155          42                      his usual bad boy weirdo role   \n",
       "1155      1156          42                          usual bad boy weirdo role   \n",
       "1156      1157          42                                              usual   \n",
       "1157      1158          42                                bad boy weirdo role   \n",
       "1158      1159          42                                                bad   \n",
       "1159      1160          42                                    boy weirdo role   \n",
       "1160      1161          42                                                boy   \n",
       "1161      1162          42                                        weirdo role   \n",
       "1162      1163          42                                             weirdo   \n",
       "1163      1164          42                                               role   \n",
       "\n",
       "      Sentiment  \n",
       "1134          3  \n",
       "1135          2  \n",
       "1136          2  \n",
       "1137          2  \n",
       "1138          3  \n",
       "1139          3  \n",
       "1140          2  \n",
       "1141          3  \n",
       "1142          3  \n",
       "1143          3  \n",
       "1144          2  \n",
       "1145          2  \n",
       "1146          2  \n",
       "1147          2  \n",
       "1148          2  \n",
       "1149          3  \n",
       "1150          2  \n",
       "1151          2  \n",
       "1152          2  \n",
       "1153          2  \n",
       "1154          2  \n",
       "1155          2  \n",
       "1156          2  \n",
       "1157          2  \n",
       "1158          0  \n",
       "1159          2  \n",
       "1160          2  \n",
       "1161          2  \n",
       "1162          2  \n",
       "1163          2  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.loc[train_data['SentenceId'] == 42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Powyższa ramka danych uświadamia, że w skład `Phrase` wchodzą zarówno jednowyrazowe napisy oraz napisy składające się z wielu wyrazów. Warto zauważyć, że niektóre `Phrase` składają się tylko z tzw. *stopwords*. Okazuje się, że frazy będące stopwords to nieznaczna część danych, więc w dalszych krokach będę używał metody wykluczania stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    0.999179\n",
       "True     0.000821\n",
       "Name: Phrase, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['Phrase'].apply(lambda x: x in stopwords.words('english')).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Czy we frazach występują znaki interpunkcyjne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    0.573704\n",
       "True     0.426296\n",
       "Name: Phrase, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['Phrase'].apply(lambda x: any([punc in x for punc in string.punctuation])).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rozkład zmiennej zależnej według fraz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdzam jaki jest rozkład zmiennej zależnej (`Sentiment`) według fraz składających się na zdania (`PhraseId`). Przeważa klasa środkowa, czyli neutralna. Rozkład podobny dla wszystkich trzech zestawów danych. Interesujące jest to, że frazy oznaczone jako skrajne(`0` oraz `4`) są średnio najdłuższe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_by_phrase_stats(set_df):\n",
    "    no_rows = set_df.shape[0]\n",
    "\n",
    "    aggregation = {\n",
    "        'Phrase': {\n",
    "            'count': 'count',\n",
    "            'freq': lambda x: x.shape[0]/no_rows,\n",
    "            'avg_number_of_words': lambda x: np.mean([len(phrase.split()) for phrase in x])\n",
    "        }    \n",
    "    }\n",
    "\n",
    "    return set_df.groupby('Sentiment').agg(aggregation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pawel/.local/lib/python3.6/site-packages/pandas/core/groupby/groupby.py:4656: FutureWarning: using a dict with renaming is deprecated and will be removed in a future version\n",
      "  return super(DataFrameGroupBy, self).aggregate(arg, *args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[          Phrase                              \n",
       "            count      freq avg_number_of_words\n",
       " Sentiment                                     \n",
       " 0           6318  0.045129           12.035296\n",
       " 1          24081  0.172008            9.076118\n",
       " 2          71738  0.512418            5.139466\n",
       " 3          29602  0.211444            8.404229\n",
       " 4           8260  0.059000           10.662228,\n",
       "           Phrase                              \n",
       "            count      freq avg_number_of_words\n",
       " Sentiment                                     \n",
       " 0            328  0.042051           13.435976\n",
       " 1           1519  0.194744            9.903226\n",
       " 2           3788  0.485641            5.714361\n",
       " 3           1696  0.217436            8.825472\n",
       " 4            469  0.060128           11.285714,\n",
       "           Phrase                              \n",
       "            count      freq avg_number_of_words\n",
       " Sentiment                                     \n",
       " 0            417  0.053462           12.177458\n",
       " 1           1605  0.205769            8.895327\n",
       " 2           3822  0.490000            5.639194\n",
       " 3           1522  0.195128            8.829172\n",
       " 4            434  0.055641           10.930876]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: get_by_phrase_stats(x), (train_data, valid_data, test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zdefiniowane zadania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W dalszych krokach będę próbował określić wartość kolumny `Sentiment` dla każdej z `Phrase`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Podejście z pełnymi zdaniami - ustalenie benchmarku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W tym podejściu skupiam się jedynie na pełnych zdaniach. Innymi słowy ignoruję podział zdania na frazy i jedynie na podstawie całego zdania próbuję określić wartość `Sentiment`. Dla fraz wchodzących w skład danego zdania przypiszę wartości `Sentiment` uzyskane dla tego zdania. Ze względu na niedoskonałość tego podejścia będę mógł je potraktować jako swego rodzaju benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W zestawie treningowym i walidacyjnym zostawiam tylko pełne zdania:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_full_sent_only = train_data.drop_duplicates(subset='SentenceId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>This quiet , introspective and entertaining in...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>82</td>\n",
       "      <td>3</td>\n",
       "      <td>Even fans of Ismail Merchant 's work , I suspe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>117</td>\n",
       "      <td>4</td>\n",
       "      <td>A positively thrilling combination of ethnogra...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>157</td>\n",
       "      <td>5</td>\n",
       "      <td>Aggressive self-glorification and a manipulati...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PhraseId  SentenceId                                             Phrase  \\\n",
       "0           1           1  A series of escapades demonstrating the adage ...   \n",
       "63         64           2  This quiet , introspective and entertaining in...   \n",
       "81         82           3  Even fans of Ismail Merchant 's work , I suspe...   \n",
       "116       117           4  A positively thrilling combination of ethnogra...   \n",
       "156       157           5  Aggressive self-glorification and a manipulati...   \n",
       "\n",
       "     Sentiment  \n",
       "0            1  \n",
       "63           4  \n",
       "81           1  \n",
       "116          3  \n",
       "156          1  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_full_sent_only.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data_full_sent_only = valid_data.drop_duplicates(subset='SentenceId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140461</td>\n",
       "      <td>7622</td>\n",
       "      <td>democracy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>140465</td>\n",
       "      <td>7623</td>\n",
       "      <td>Griffin &amp; Co. manage to be spectacularly outra...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>140475</td>\n",
       "      <td>7624</td>\n",
       "      <td>Like The English Patient and The Unbearable Li...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>140510</td>\n",
       "      <td>7625</td>\n",
       "      <td>`` Auto Focus '' works as an unusual biopic an...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>140529</td>\n",
       "      <td>7626</td>\n",
       "      <td>Very amusing , not the usual route in a thrill...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    PhraseId  SentenceId                                             Phrase  \\\n",
       "0     140461        7622                                          democracy   \n",
       "4     140465        7623  Griffin & Co. manage to be spectacularly outra...   \n",
       "14    140475        7624  Like The English Patient and The Unbearable Li...   \n",
       "49    140510        7625  `` Auto Focus '' works as an unusual biopic an...   \n",
       "68    140529        7626  Very amusing , not the usual route in a thrill...   \n",
       "\n",
       "    Sentiment  \n",
       "0           2  \n",
       "4           4  \n",
       "14          3  \n",
       "49          3  \n",
       "68          3  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data_full_sent_only.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniższa funkcja sprowadza dane zdanie do mniej złożonej reprezentacji (nadal literowej):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "def simplify_sentence(sentence):\n",
    "    # usuwanie znaków specjalnych (wszystkich poza alfanumerycznymi)\n",
    "    sentence = re.sub(r'\\W', ' ', sentence)\n",
    "    \n",
    "    # usuwanie wszystkich pojedynczych liter\n",
    "    sentence = re.sub('(\\\\b[A-Za-z] \\\\b|\\\\b [A-Za-z]\\\\b)', '', sentence)\n",
    "\n",
    "    # zamiana na małe litery\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # wyciągnięcie pojedynczych słów\n",
    "    sentence = sentence.split()\n",
    "    \n",
    "    # sprowadzenie słów do form podstawowych (lematów)\n",
    "    # uwaga: sentence to teraz lista\n",
    "    sentence = [stemmer.lemmatize(word) for word in sentence]\n",
    "    \n",
    "    # powrót do str\n",
    "    return \" \".join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pawel/.local/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "train_data_full_sent_only['Phrase_cleaned'] = train_data_full_sent_only['Phrase'].apply(lambda x: simplify_sentence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pawel/.local/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "valid_data_full_sent_only['Phrase_cleaned'] = valid_data_full_sent_only['Phrase'].apply(lambda x: simplify_sentence(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zamieniam uproszczone (wyczyszczone zdania) na reprezentację bag of words. \n",
    "\n",
    "`max_features`: zwracam uwagę tylko na 1000 najczęściej występujących słów, \n",
    "\n",
    "`min_df` to liczba minimalna liczba zdań, w których musi wystąpić słowo, żeby nie zostać zignorowanym, \n",
    "\n",
    "`max_df` to maksymalny odsetek wszystkich zdań, w których słowo może wystąpić - służy do zignorowania słów, które występują w prawie każdym zdaniu\n",
    "\n",
    "`stop_words` to słowa, które występują na tyle często w języku, że nie wnoszą żadnych informacji w zadaniu klasyfikacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=30, min_df=5, max_df=0.75, stop_words=stopwords.words('english'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_vectorized = vectorizer.fit_transform(train_data_full_sent_only['Phrase_cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data_vectorized = vectorizer.fit_transform(valid_data_full_sent_only['Phrase_cleaned'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dotychczasowa reprezentacja wektorowa zdań mimo że uwzględnia fakt bardzo rzadkich słów (`min_df`) oraz słów występujących w prawie każdym zdaniu (`max_df`) może zostać ulepszona podejściem TFIDF. Podejście to nadaje większą wagę słowom bardziej informatywnym, tzn. tym które występują często w małej ilości zdań."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfconverter = TfidfTransformer()  \n",
    "train_data_tfidf = tfidfconverter.fit_transform(train_data_vectorized).toarray() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data_tfidf = tfidfconverter.fit_transform(valid_data_vectorized).toarray() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Używam lasu losowego z 500 drzewami w celu dokonania klasyfikacji:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators=500, random_state=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_classifier.fit(train_data_tfidf, train_data_full_sent_only['Sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przygotowuję dane testowe, na podstawie których dokonam predykcji:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_full_sent_only = test_data.drop_duplicates(subset='SentenceId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pawel/.local/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "test_data_full_sent_only['Phrase_cleaned'] = test_data_full_sent_only['Phrase'].apply(lambda x: simplify_sentence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_vectorize = vectorizer.fit_transform(test_data_full_sent_only['Phrase_cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_tfidf = tfidfconverter.fit_transform(test_data_vectorize).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_classifier.predict(test_data_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`y_pred` muszą zostać skopiowane do odpowiadających `PhraseId`, aby można było obliczyć dokładność klasyfikatora. Z jednej strony mam tyle unikalnych `SentenceId` w `test_data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "475"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... z drugiej strony w `test_data` jest tyle unikalnych `PhraseId`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7800"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wykonuję złączenie dwóch ramek danych po `SentenceId`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_only_preds = pd.DataFrame(data={\"predicted_class\": y_pred, \"SentenceId\": test_data_full_sent_only['SentenceId']}, index=test_data_full_sent_only.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_class</th>\n",
       "      <th>SentenceId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>8068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>8069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>8070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>4</td>\n",
       "      <td>8071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1</td>\n",
       "      <td>8072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    predicted_class  SentenceId\n",
       "0                 3        8068\n",
       "5                 1        8069\n",
       "23                3        8070\n",
       "61                4        8071\n",
       "71                1        8072"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_only_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>predicted_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148261</td>\n",
       "      <td>8068</td>\n",
       "      <td>feminist conspiracy theorist</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>148262</td>\n",
       "      <td>8068</td>\n",
       "      <td>conspiracy theorist</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>148263</td>\n",
       "      <td>8068</td>\n",
       "      <td>theorist</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>148264</td>\n",
       "      <td>8068</td>\n",
       "      <td>named Dirty Dick</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>148265</td>\n",
       "      <td>8068</td>\n",
       "      <td>Dirty Dick</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                        Phrase  Sentiment  \\\n",
       "0    148261        8068  feminist conspiracy theorist          2   \n",
       "1    148262        8068           conspiracy theorist          2   \n",
       "2    148263        8068                      theorist          2   \n",
       "3    148264        8068              named Dirty Dick          2   \n",
       "4    148265        8068                    Dirty Dick          2   \n",
       "\n",
       "   predicted_class  \n",
       "0                3  \n",
       "1                3  \n",
       "2                3  \n",
       "3                3  \n",
       "4                3  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_w_predicted = pd.merge(test_data, sent_only_preds, how='left', on='SentenceId')\n",
    "test_data_w_predicted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biorąc pod uwagę tragiczne wyniki klasyfikatora za benchmark w dalszych rozważaniach będę uznawał częstość występowania klasy dominującej w zadaniach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20717948717948717\n",
      "Benchmark dla accuracy będzie wynosił:  0.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pawel/.local/lib/python3.6/site-packages/pandas/core/groupby/groupby.py:4656: FutureWarning: using a dict with renaming is deprecated and will be removed in a future version\n",
      "  return super(DataFrameGroupBy, self).aggregate(arg, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(test_data_w_predicted['Sentiment'], test_data_w_predicted['predicted_class']))\n",
    "print(\"Benchmark dla accuracy będzie wynosił: \", max(get_by_phrase_stats(test_data)['Phrase']['freq']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelowanie embeddingsów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W dalszej części analizy będę posługiwał się podejściem z uczenia głębokiego. W przypadku modelowania tekstu potrzebne jest utworzenie wektorów zanurzonych. Na początku spróbuję utworzyć je własnoręcznie, później skorzystam z już obliczonych, publicznie dostępnych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trenowanie własnych embeddingsów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniżej korzystam z podejścia word2vec. Jako, że mam pięć kategorii w `Sentiment` to utworzę embeddingi dla 5 przypadków."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcja czyszcząca zdania. Mimo wcześniejszego zdefiniowania funkcji służącej wyczyszczeniu tekstu tutaj korzystam z funkcji podanej w dedykowanym word2vec notebook'owi z zajęć:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcja dodająca słowa do słownika:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_doc_to_vocab(sent, vocab):\n",
    "    tokens = clean_doc(sent)\n",
    "    vocab.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = dict()\n",
    "\n",
    "for sentiment_value in train_data.Sentiment.unique():\n",
    "    tmp_vocab = Counter()\n",
    "    tmp_df = train_data.loc[train_data['Sentiment']==sentiment_value]\n",
    "    for phrase in tmp_df['Phrase']:\n",
    "        add_doc_to_vocab(phrase, tmp_vocab)\n",
    "    vocabs[\"sentiment_{}\".format(sentiment_value)] = tmp_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_1\n",
      "sentiment_2\n",
      "sentiment_3\n",
      "sentiment_4\n",
      "sentiment_0\n"
     ]
    }
   ],
   "source": [
    "for key in vocabs.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdzam najczęstsze słowa dla każdej wartości `Sentiment`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common  sentiment_1 [('nt', 1368), ('movie', 1216), ('film', 1078), ('The', 925), ('like', 739), ('one', 578), ('much', 526), ('RRB', 447), ('little', 429), ('story', 409)]\n",
      "Least common  sentiment_1 [('JFK', 1), ('nuts', 1), ('soulsearching', 1), ('deliberateness', 1), ('deny', 1), ('seriousness', 1), ('flatout', 1), ('pleased', 1), ('matineestyle', 1), ('restrictive', 1)]\n",
      "Most common  sentiment_2 [('film', 1919), ('movie', 1652), ('The', 1635), ('nt', 1131), ('one', 1085), ('RRB', 989), ('like', 926), ('story', 808), ('LRB', 780), ('much', 608)]\n",
      "Least common  sentiment_2 [('transfixes', 1), ('freakout', 1), ('Songs', 1), ('During', 1), ('boorishness', 1), ('Oscarworthy', 1), ('scorcher', 1), ('fearlessly', 1), ('deny', 1), ('bangup', 1)]\n",
      "Most common  sentiment_3 [('film', 1619), ('movie', 1147), ('The', 945), ('good', 768), ('one', 698), ('story', 603), ('funny', 579), ('nt', 517), ('RRB', 494), ('like', 464)]\n",
      "Least common  sentiment_3 [('artconscious', 1), ('Accidental', 1), ('Everyone', 1), ('developers', 1), ('Commerce', 1), ('tourism', 1), ('pageants', 1), ('Beneath', 1), ('mornings', 1), ('Katz', 1)]\n",
      "Most common  sentiment_4 [('film', 848), ('movie', 488), ('one', 410), ('The', 338), ('best', 332), ('funny', 319), ('good', 261), ('performances', 222), ('story', 220), ('comedy', 205)]\n",
      "Least common  sentiment_4 [('Pal', 1), ('HG', 1), ('Wells', 1), ('dreamed', 1), ('probes', 1), ('Iran', 1), ('Afghani', 1), ('refugees', 1), ('streamed', 1), ('borders', 1)]\n",
      "Most common  sentiment_0 [('movie', 676), ('film', 432), ('nt', 382), ('bad', 378), ('The', 296), ('like', 259), ('one', 222), ('minutes', 149), ('even', 147), ('characters', 146)]\n",
      "Least common  sentiment_0 [('Kaos', 1), ('blown', 1), ('neglecting', 1), ('endorsement', 1), ('abhors', 1), ('Storytelling', 1), ('dance', 1), ('completists', 1), ('restrictive', 1), ('mornings', 1)]\n"
     ]
    }
   ],
   "source": [
    "for senti_value, cnt in vocabs.items():\n",
    "    print(\"Most common \", senti_value, cnt.most_common(10))\n",
    "    print(\"Least common \", senti_value, cnt.most_common()[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ze słownika usunę te słowa, które wystąpiły tylko raz. Najpierw tworzę słownik dla całego zestawu treningowe łącząc słownika dla poszczególnych wartości `Sentiment`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_vocab = sum(list(vocabs.values()), Counter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', 5896), ('movie', 5179), ('The', 4139), ('nt', 3520), ('one', 2993)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_vocab.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_occurence = 2\n",
    "tokens_more_than_once = [k for k, v in complete_vocab.items() if v >= min_occurence]\n",
    "vocab_more_than_once = set(tokens_more_than_once) #dostęp O(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kolejnym krokiem jest przefiltrowanie danych treningowych pod kątem słów, które zebrałem w słowniku. Definiuję funkcję, która poza podstawowym czyszczeniem sprawdza także czy słowo należy do słów ze słownika:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc_with_vocab(phrase, vocab):\n",
    "    # split into tokens by white space\n",
    "    tokens = phrase.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # filter out tokens not in vocab\n",
    "    tokens = [w for w in tokens if w in vocab] # sprawdzamy czy tokeny występują w słowniku\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs = []\n",
    "for phrase in train_data['Phrase']:\n",
    "    res = clean_doc_with_vocab(phrase, vocab_more_than_once)\n",
    "    if res != '':\n",
    "        train_docs.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['series escapades demonstrating adage good goose also good gander occasionally amuses none amounts much story',\n",
       " 'series escapades demonstrating adage good goose',\n",
       " 'series',\n",
       " 'series',\n",
       " 'escapades demonstrating adage good goose',\n",
       " 'escapades demonstrating adage good goose',\n",
       " 'escapades',\n",
       " 'demonstrating adage good goose',\n",
       " 'demonstrating adage',\n",
       " 'demonstrating']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tworzę tokenizer, który zmapuje słowa na liczby:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_docs)\n",
    "encoded_docs = tokenizer.texts_to_sequences(train_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdzam jak wygląda `tokenizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zmieniła się struktura danych, które do tej pory były ciągem słów i spacji:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quiet introspective entertaining independent\n"
     ]
    }
   ],
   "source": [
    "print(train_docs[51])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz struktura to ciąg liczb oddzielonych przecinkiem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[661, 4457, 99, 3285]\n"
     ]
    }
   ],
   "source": [
    "print(encoded_docs[51])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sieć neuronowa, której produktem ubocznym są embeddingi wymaga, aby wejście było stałej długości. Biorę najdłuższy element i wyrównuję pozostałe elementy do jego długości, wstawiając w puste miejsca `0` (puste miejsca występują po właściwej zawartości):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max([len(s.split()) for s in train_docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najdłuższy ciąg słów ma długość:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = pad_sequences(encoded_docs, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okazuje się, że wymiar `encoded_docs` nie zgadza się z wymiarem pierwotnej ramki danych `train_data`, co utrudnia dodanie zmiennej zależnej. Sytuacja spowodowana jest tym, że niektóre obserwacje z `train_data` zawierały same stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138616"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(139999, 4)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poddaję ramkę danych `train_data` (kolumnę `Phrase`) działaniu funkcji `clean_doc()` a następnie usuwam wiersze, gdzie `Phrase` jest pustym ciągiem znaków:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_clean_phrase = train_data.copy(deep=True)\n",
    "train_data_clean_phrase['Phrase'] = train_data['Phrase'].apply(lambda x: \" \".join(clean_doc(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Widać, że niektóre `Phrase` są teraz puste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>series escapades demonstrating adage good goos...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>series escapades demonstrating adage good goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  series escapades demonstrating adage good goos...   \n",
       "1         2           1    series escapades demonstrating adage good goose   \n",
       "2         3           1                                             series   \n",
       "3         4           1                                                      \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_clean_phrase.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139999"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_clean_phrase.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usuwam puste `Phrase`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_clean_phrase = train_data_clean_phrase.drop(train_data_clean_phrase[train_data_clean_phrase.Phrase == \"\"].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138650"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_clean_phrase.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rozmiar ciągle się nie zgadza, ponieważ użyłem funkcji `clean_doc_with_vocab()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_clean_phrase['Phrase'] = train_data_clean_phrase['Phrase'].apply(lambda x: clean_doc_with_vocab(x, vocab_more_than_once))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usuwam puste wiersze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_clean_phrase = train_data_clean_phrase.drop(train_data_clean_phrase[train_data_clean_phrase.Phrase == \"\"].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138616"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_clean_phrase.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz mogę bezpiecznie przekopiować `Sentiment` z ramki danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.array(train_data_clean_phrase['Sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Czynności dotyczące obróbki tekstu i zamiany na liczby powtarzam dla zestawu testowego. Tym razem tekst obrabiam wewnątrz ramki danych, żeby nie stracić informacji, które wiersze zostały z niej usunięte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_clean_phrase = test_data.copy(deep=True)\n",
    "test_data_clean_phrase['Phrase'] = test_data_clean_phrase['Phrase'].apply(lambda x: \" \".join(clean_doc(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_clean_phrase['Phrase'] = test_data_clean_phrase['Phrase'].apply(lambda x: clean_doc_with_vocab(x, vocab_more_than_once))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_clean_phrase = test_data_clean_phrase.drop(test_data_clean_phrase[test_data_clean_phrase.Phrase == \"\"].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148261</td>\n",
       "      <td>8068</td>\n",
       "      <td>feminist conspiracy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>148262</td>\n",
       "      <td>8068</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>148264</td>\n",
       "      <td>8068</td>\n",
       "      <td>named Dirty Dick</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>148265</td>\n",
       "      <td>8068</td>\n",
       "      <td>Dirty Dick</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>148266</td>\n",
       "      <td>8069</td>\n",
       "      <td>The action XXX blast excitement</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                           Phrase  Sentiment\n",
       "0    148261        8068              feminist conspiracy          2\n",
       "1    148262        8068                       conspiracy          2\n",
       "3    148264        8068                 named Dirty Dick          2\n",
       "4    148265        8068                       Dirty Dick          2\n",
       "5    148266        8069  The action XXX blast excitement          4"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_clean_phrase.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs = test_data_clean_phrase['Phrase'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feminist conspiracy',\n",
       " 'conspiracy',\n",
       " 'named Dirty Dick',\n",
       " 'Dirty Dick',\n",
       " 'The action XXX blast excitement']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_test_docs = tokenizer.texts_to_sequences(test_docs) # słowa jako liczby\n",
    "test_X = pad_sequences(encoded_docs, maxlen=max_len, padding='post') # stała długość wejścia\n",
    "test_y = np.array(test_data_clean_phrase['Sentiment']) # label danych testowych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potrzebny jest rozmiar słownika. Pamiętam o zostawieniu oddzielnego znaku dla paddingu wykorzystanego w wyrównywaniu do stałej długości wejścia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15167"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_size = len(tokenizer.word_index) + 1\n",
    "v_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W celu stworzenia embeddingsów korzystam z sieci z zajęć:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 30, 20)            303340    \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 23, 32)            5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 11, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 352)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                3530      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 312,033\n",
      "Trainable params: 312,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      " - 13s - loss: -1.6860e+01 - acc: 0.1735\n",
      "Epoch 2/10\n",
      " - 13s - loss: -1.7019e+01 - acc: 0.1735\n",
      "Epoch 3/10\n",
      " - 13s - loss: -1.7019e+01 - acc: 0.1735\n",
      "Epoch 4/10\n",
      " - 13s - loss: -1.7019e+01 - acc: 0.1735\n",
      "Epoch 5/10\n",
      " - 13s - loss: -1.7020e+01 - acc: 0.1736\n",
      "Epoch 6/10\n",
      " - 13s - loss: -1.7020e+01 - acc: 0.1736\n",
      "Epoch 7/10\n",
      " - 14s - loss: -1.7021e+01 - acc: 0.1736\n",
      "Epoch 8/10\n",
      " - 14s - loss: -1.7025e+01 - acc: 0.1738\n",
      "Epoch 9/10\n",
      " - 13s - loss: -1.7027e+01 - acc: 0.1739\n",
      "Epoch 10/10\n",
      " - 13s - loss: -1.7029e+01 - acc: 0.1739\n",
      "Test Accuracy: 17.402753\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(v_size, 20, input_length=max_len)) # 100-wymiarowy embedding - \n",
    "                                                               # wektor zanurzony o długości 100\n",
    "model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(train_X, train_y, epochs=10, verbose=2)\n",
    "# loss, acc = model.evaluate(test_X, test_y, verbose=0)\n",
    "loss, acc = model.evaluate(train_X, train_y, verbose=0) #sprawdzam czy loss zmienia się na danych treningowych - powinno przechodzić do 1\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  216, 13938,  5295, ...,     0,     0,     0],\n",
       "       [  216, 13938,  5295, ...,     0,     0,     0],\n",
       "       [  216,     0,     0, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  183,   339,  3050, ...,     0,     0,     0],\n",
       "       [    5,   379,     2, ...,     0,     0,     0],\n",
       "       [    5,   379,     2, ...,     0,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeggings - 24.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Czyszczenie danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniżej zdefiniowana funkcja jest zbyt agresywna, ponieważ dla niektórych obserwacji w `Phrase` (np. tych zawierających same *stopwords*) będzie zwracała puste napisy. W jej miejsce stosuję wyrażenie regularne, które usuwa wszystkie znaki poza literami łacińskimi (małymi i wielkimi), cyframi oraz odstępami (np. spacje, taby, nowe linie). Mimo, że wykorzystany w dalszej części obiekt `Tokenizer` filtruje po znakach m.in. znakach odstępu (`\\s`) usuwając je, to na razie muszę zostawić odstępy, które stanowią granicę wyrazów (tokenów) w zdaniu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Phrase'] = train_data['Phrase'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['Phrase'] = test_data['Phrase'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148261</td>\n",
       "      <td>8068</td>\n",
       "      <td>feminist conspiracy theorist</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>148262</td>\n",
       "      <td>8068</td>\n",
       "      <td>conspiracy theorist</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>148263</td>\n",
       "      <td>8068</td>\n",
       "      <td>theorist</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>148264</td>\n",
       "      <td>8068</td>\n",
       "      <td>named Dirty Dick</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>148265</td>\n",
       "      <td>8068</td>\n",
       "      <td>Dirty Dick</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                        Phrase  Sentiment\n",
       "0    148261        8068  feminist conspiracy theorist          2\n",
       "1    148262        8068           conspiracy theorist          2\n",
       "2    148263        8068                      theorist          2\n",
       "3    148264        8068              named Dirty Dick          2\n",
       "4    148265        8068                    Dirty Dick          2"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na wejściu do sieci musi pojawiać się wektor stałej długości. Dodatkowym wymaganiem jest to, żeby słowa w nim zawarte były zakodowane jako liczby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = train_data['Phrase']\n",
    "y_tr = train_data['Sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiuję `Tokenizer`, którego jednym z argumentów jest m.in. to ile słów z wejścia zachowa. Domyślne ustawienia (`num_words`) to `num_words-1`, co oznacza, że słowa o najniższej częstości występowania nie przejdą etapu tokenizacji. Wydaje się to rozsądnym pomysłem, ponieważ słowa najrzadziej występujące to zazwyczaj te występujące tylko raz. Dodatkowo przerabiam wszystkie znaki na wersje pisane małą literą (argument `lower=True`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W tym kroku tworzę słownik, który ma następującą postać `słownik['the']=1`, czyli każde słowo z zestawu treningowego jest kodowane jako liczba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(X_tr.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ts = test_data['Phrase']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniżej elementy zdania zamieniane są na liczby:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_tokenized = tokenizer.texts_to_sequences(X_tr)\n",
    "X_ts_tokenized = tokenizer.texts_to_sequences(X_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniższy przykład pokazuje, że zdania zostały zamienione na listy długości równej liczbie tokenów w pierwotnych (ale oczyszczonych regexem) zdaniach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('to much of a story', [5, 54, 3, 2, 40])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr[55], X_tr_tokenized[55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('for excitement', [13, 1472])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ts[22], X_ts_tokenized[22]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Należy wyrównać listy do tej samej długości (długości najliczniejszego pod względem tokenów zdania). Argument `padding='post'` wskazuje, że w krótszych niż `max_len` zdaniach zera będą dodane na końcu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = max([len(x.split()) for x in train_data['Phrase']])\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_tokenized = pad_sequences(X_tr_tokenized, max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   2,  316,    3, ...,    0,    0,    0],\n",
       "       [   2,  316,    3, ...,    0,    0,    0],\n",
       "       [   2,  316,    0, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 280,  442, 3148, ...,    0,    0,    0],\n",
       "       [   8,   28,  482, ...,    0,    0,    0],\n",
       "       [  28,  482,   17, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ts_tokenized = pad_sequences(X_ts_tokenized, max_len, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Długość tokenizowanych zdań w obu zestawach jest równa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_tokenized.shape[1] == X_ts_tokenized.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Budowa sieci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Każdy z wektorów zanurzonych w sieci będzie miał wymiar 100. Zgodnie z dokumentacją `input_dim` dla w `Embedding()` musi być równy liczbie słów plus 1 (plus 1, aby ująć `0` za pomocą, którego wypełnia się wektory krótsze niż maksymalny wymiar). `input_length` to argument informujący o długości wejścia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dim = 100\n",
    "input_dim = len(tokenizer.word_index)+1\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=input_dim, output_dim=embeddings_dim, input_length=max_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warstwa Long Short Term Memory (`LSTM`) oraz warstawa klasyfikująca `Dense`, w której umieszczam 5 neuronów (jedna dla każdej z przewidywanych klas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(LSTM(units=128))\n",
    "model.add(Dense(units=1, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warto zwrócić uwagę na funkcję straty `'sparse_categorical_crossentropy'`, która powinna być używana w przypadku, gdy klasy są rozłączne. W naszym przypadku oznacza to, że ostatnia warstwa nie zwraca prawdopodobieństw należenia do różnych klas. Wspomniana funkcja jest używana, gdy zmienna zależna nie jest kodowana w postaci one-hot, lecz jako liczby całkowite (np. 1, 2, 3, 4, 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae', 'acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 48, 100)           1540600   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,657,977\n",
      "Trainable params: 1,657,977\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pawel/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "139999/139999 [==============================] - 207s 1ms/step - loss: 1.9343 - mean_absolute_error: 1.1574 - acc: 0.1720\n",
      "Epoch 2/2\n",
      "139999/139999 [==============================] - 221s 2ms/step - loss: 1.9343 - mean_absolute_error: 1.1574 - acc: 0.1720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f22a71676d8>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_tr_tokenized, y_tr, epochs = 2, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notatki\n",
    "1. Wykorzystać FastText crawl 300d 2M."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
