{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.utils import class_weight\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [Errno -2] Name or\n",
      "[nltk_data]     service not known>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno -2] Name or\n",
      "[nltk_data]     service not known>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Załadowanie plików z danymi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pliki z danymi znajduję się w folderze `./data/`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.tsv  train.tsv  valid.tsv\n"
     ]
    }
   ],
   "source": [
    "!ls ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./data/train.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W danych walidacyjnych i testowych brakuje nagłówków, więc dodaję je z pliku treningowego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = pd.read_csv('./data/valid.tsv', sep='\\t', header=None, names=train_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140461</td>\n",
       "      <td>7622</td>\n",
       "      <td>democracy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>140462</td>\n",
       "      <td>7622</td>\n",
       "      <td>and civic action laudable</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>140463</td>\n",
       "      <td>7622</td>\n",
       "      <td>civic action laudable</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>140464</td>\n",
       "      <td>7622</td>\n",
       "      <td>action laudable</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>140465</td>\n",
       "      <td>7623</td>\n",
       "      <td>Griffin &amp; Co. manage to be spectacularly outra...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0    140461        7622                                          democracy   \n",
       "1    140462        7622                          and civic action laudable   \n",
       "2    140463        7622                              civic action laudable   \n",
       "3    140464        7622                                    action laudable   \n",
       "4    140465        7623  Griffin & Co. manage to be spectacularly outra...   \n",
       "\n",
       "   Sentiment  \n",
       "0          2  \n",
       "1          3  \n",
       "2          2  \n",
       "3          3  \n",
       "4          4  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7795</th>\n",
       "      <td>148256</td>\n",
       "      <td>8068</td>\n",
       "      <td>a storm</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7796</th>\n",
       "      <td>148257</td>\n",
       "      <td>8068</td>\n",
       "      <td>as a fringe feminist conspiracy theorist</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7797</th>\n",
       "      <td>148258</td>\n",
       "      <td>8068</td>\n",
       "      <td>a fringe feminist conspiracy theorist</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7798</th>\n",
       "      <td>148259</td>\n",
       "      <td>8068</td>\n",
       "      <td>fringe feminist conspiracy theorist</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7799</th>\n",
       "      <td>148260</td>\n",
       "      <td>8068</td>\n",
       "      <td>fringe</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      PhraseId  SentenceId                                    Phrase  \\\n",
       "7795    148256        8068                                   a storm   \n",
       "7796    148257        8068  as a fringe feminist conspiracy theorist   \n",
       "7797    148258        8068     a fringe feminist conspiracy theorist   \n",
       "7798    148259        8068       fringe feminist conspiracy theorist   \n",
       "7799    148260        8068                                    fringe   \n",
       "\n",
       "      Sentiment  \n",
       "7795          2  \n",
       "7796          1  \n",
       "7797          2  \n",
       "7798          1  \n",
       "7799          2  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('./data/test.tsv', sep='\\t', header=None, names=train_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148261</td>\n",
       "      <td>8068</td>\n",
       "      <td>feminist conspiracy theorist</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>148262</td>\n",
       "      <td>8068</td>\n",
       "      <td>conspiracy theorist</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>148263</td>\n",
       "      <td>8068</td>\n",
       "      <td>theorist</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>148264</td>\n",
       "      <td>8068</td>\n",
       "      <td>named Dirty Dick</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>148265</td>\n",
       "      <td>8068</td>\n",
       "      <td>Dirty Dick</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                        Phrase  Sentiment\n",
       "0    148261        8068  feminist conspiracy theorist          2\n",
       "1    148262        8068           conspiracy theorist          2\n",
       "2    148263        8068                      theorist          2\n",
       "3    148264        8068              named Dirty Dick          2\n",
       "4    148265        8068                    Dirty Dick          2"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7795</th>\n",
       "      <td>156056</td>\n",
       "      <td>8544</td>\n",
       "      <td>Hearst 's</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7796</th>\n",
       "      <td>156057</td>\n",
       "      <td>8544</td>\n",
       "      <td>forced avuncular chortles</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7797</th>\n",
       "      <td>156058</td>\n",
       "      <td>8544</td>\n",
       "      <td>avuncular chortles</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7798</th>\n",
       "      <td>156059</td>\n",
       "      <td>8544</td>\n",
       "      <td>avuncular</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7799</th>\n",
       "      <td>156060</td>\n",
       "      <td>8544</td>\n",
       "      <td>chortles</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      PhraseId  SentenceId                     Phrase  Sentiment\n",
       "7795    156056        8544                  Hearst 's          2\n",
       "7796    156057        8544  forced avuncular chortles          1\n",
       "7797    156058        8544         avuncular chortles          3\n",
       "7798    156059        8544                  avuncular          2\n",
       "7799    156060        8544                   chortles          2"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdzam czy istnieją komórki z wartością `NaN`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False, False]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: x.isnull().any().any(), (train_data, valid_data, test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wstępna analiza danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forma i wygląd danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zdania oznaczone przez `SentenceId` są podzielone na frazy (`Phrase`), z których każda posiada określony `Sentiment`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1134</th>\n",
       "      <td>1135</td>\n",
       "      <td>42</td>\n",
       "      <td>Vincent Gallo is right at home in this French ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>1136</td>\n",
       "      <td>42</td>\n",
       "      <td>Vincent Gallo</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136</th>\n",
       "      <td>1137</td>\n",
       "      <td>42</td>\n",
       "      <td>Vincent</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>1138</td>\n",
       "      <td>42</td>\n",
       "      <td>Gallo</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>1139</td>\n",
       "      <td>42</td>\n",
       "      <td>is right at home in this French shocker playin...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>1140</td>\n",
       "      <td>42</td>\n",
       "      <td>is right at home in this French shocker playin...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140</th>\n",
       "      <td>1141</td>\n",
       "      <td>42</td>\n",
       "      <td>is right at home in this French shocker</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1141</th>\n",
       "      <td>1142</td>\n",
       "      <td>42</td>\n",
       "      <td>right at home in this French shocker</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>1143</td>\n",
       "      <td>42</td>\n",
       "      <td>right at home</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143</th>\n",
       "      <td>1144</td>\n",
       "      <td>42</td>\n",
       "      <td>right</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <td>1145</td>\n",
       "      <td>42</td>\n",
       "      <td>at home</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1145</th>\n",
       "      <td>1146</td>\n",
       "      <td>42</td>\n",
       "      <td>at</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1146</th>\n",
       "      <td>1147</td>\n",
       "      <td>42</td>\n",
       "      <td>home</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>1148</td>\n",
       "      <td>42</td>\n",
       "      <td>in this French shocker</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>1149</td>\n",
       "      <td>42</td>\n",
       "      <td>this French shocker</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149</th>\n",
       "      <td>1150</td>\n",
       "      <td>42</td>\n",
       "      <td>French shocker</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>1151</td>\n",
       "      <td>42</td>\n",
       "      <td>French</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>1152</td>\n",
       "      <td>42</td>\n",
       "      <td>shocker</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152</th>\n",
       "      <td>1153</td>\n",
       "      <td>42</td>\n",
       "      <td>playing his usual bad boy weirdo role</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1153</th>\n",
       "      <td>1154</td>\n",
       "      <td>42</td>\n",
       "      <td>playing</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>1155</td>\n",
       "      <td>42</td>\n",
       "      <td>his usual bad boy weirdo role</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>1156</td>\n",
       "      <td>42</td>\n",
       "      <td>usual bad boy weirdo role</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>1157</td>\n",
       "      <td>42</td>\n",
       "      <td>usual</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <td>1158</td>\n",
       "      <td>42</td>\n",
       "      <td>bad boy weirdo role</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1158</th>\n",
       "      <td>1159</td>\n",
       "      <td>42</td>\n",
       "      <td>bad</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>1160</td>\n",
       "      <td>42</td>\n",
       "      <td>boy weirdo role</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>1161</td>\n",
       "      <td>42</td>\n",
       "      <td>boy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>1162</td>\n",
       "      <td>42</td>\n",
       "      <td>weirdo role</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1162</th>\n",
       "      <td>1163</td>\n",
       "      <td>42</td>\n",
       "      <td>weirdo</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>1164</td>\n",
       "      <td>42</td>\n",
       "      <td>role</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      PhraseId  SentenceId                                             Phrase  \\\n",
       "1134      1135          42  Vincent Gallo is right at home in this French ...   \n",
       "1135      1136          42                                      Vincent Gallo   \n",
       "1136      1137          42                                            Vincent   \n",
       "1137      1138          42                                              Gallo   \n",
       "1138      1139          42  is right at home in this French shocker playin...   \n",
       "1139      1140          42  is right at home in this French shocker playin...   \n",
       "1140      1141          42            is right at home in this French shocker   \n",
       "1141      1142          42               right at home in this French shocker   \n",
       "1142      1143          42                                      right at home   \n",
       "1143      1144          42                                              right   \n",
       "1144      1145          42                                            at home   \n",
       "1145      1146          42                                                 at   \n",
       "1146      1147          42                                               home   \n",
       "1147      1148          42                             in this French shocker   \n",
       "1148      1149          42                                this French shocker   \n",
       "1149      1150          42                                     French shocker   \n",
       "1150      1151          42                                             French   \n",
       "1151      1152          42                                            shocker   \n",
       "1152      1153          42              playing his usual bad boy weirdo role   \n",
       "1153      1154          42                                            playing   \n",
       "1154      1155          42                      his usual bad boy weirdo role   \n",
       "1155      1156          42                          usual bad boy weirdo role   \n",
       "1156      1157          42                                              usual   \n",
       "1157      1158          42                                bad boy weirdo role   \n",
       "1158      1159          42                                                bad   \n",
       "1159      1160          42                                    boy weirdo role   \n",
       "1160      1161          42                                                boy   \n",
       "1161      1162          42                                        weirdo role   \n",
       "1162      1163          42                                             weirdo   \n",
       "1163      1164          42                                               role   \n",
       "\n",
       "      Sentiment  \n",
       "1134          3  \n",
       "1135          2  \n",
       "1136          2  \n",
       "1137          2  \n",
       "1138          3  \n",
       "1139          3  \n",
       "1140          2  \n",
       "1141          3  \n",
       "1142          3  \n",
       "1143          3  \n",
       "1144          2  \n",
       "1145          2  \n",
       "1146          2  \n",
       "1147          2  \n",
       "1148          2  \n",
       "1149          3  \n",
       "1150          2  \n",
       "1151          2  \n",
       "1152          2  \n",
       "1153          2  \n",
       "1154          2  \n",
       "1155          2  \n",
       "1156          2  \n",
       "1157          2  \n",
       "1158          0  \n",
       "1159          2  \n",
       "1160          2  \n",
       "1161          2  \n",
       "1162          2  \n",
       "1163          2  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.loc[train_data['SentenceId'] == 42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Powyższa ramka danych uświadamia, że w skład `Phrase` wchodzą zarówno jednowyrazowe napisy oraz napisy składające się z wielu wyrazów. Warto zauważyć, że niektóre `Phrase` składają się tylko z tzw. *stopwords*. Okazuje się, że frazy będące stopwords to nieznaczna część danych, więc w dalszych krokach będę używał metody wykluczania stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    0.999179\n",
       "True     0.000821\n",
       "Name: Phrase, dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['Phrase'].apply(lambda x: x in stopwords.words('english')).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Czy we frazach występują znaki interpunkcyjne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    0.573704\n",
       "True     0.426296\n",
       "Name: Phrase, dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['Phrase'].apply(lambda x: any([punc in x for punc in string.punctuation])).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rozkład zmiennej zależnej według fraz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdzam jaki jest rozkład zmiennej zależnej (`Sentiment`) według fraz składających się na zdania (`PhraseId`). Przeważa klasa środkowa, czyli neutralna. Rozkład podobny dla wszystkich trzech zestawów danych. Interesujące jest to, że frazy oznaczone jako skrajne(`0` oraz `4`) są średnio najdłuższe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_by_phrase_stats(set_df):\n",
    "    no_rows = set_df.shape[0]\n",
    "\n",
    "    aggregation = {\n",
    "        'Phrase': {\n",
    "            'count': 'count',\n",
    "            'freq': lambda x: x.shape[0]/no_rows,\n",
    "            'avg_number_of_words': lambda x: np.mean([len(phrase.split()) for phrase in x])\n",
    "        }    \n",
    "    }\n",
    "\n",
    "    return set_df.groupby('Sentiment').agg(aggregation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pawel/.local/lib/python3.6/site-packages/pandas/core/groupby/groupby.py:4656: FutureWarning: using a dict with renaming is deprecated and will be removed in a future version\n",
      "  return super(DataFrameGroupBy, self).aggregate(arg, *args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[          Phrase                              \n",
       "            count      freq avg_number_of_words\n",
       " Sentiment                                     \n",
       " 0           6318  0.045129           12.035296\n",
       " 1          24081  0.172008            9.076118\n",
       " 2          71738  0.512418            5.139466\n",
       " 3          29602  0.211444            8.404229\n",
       " 4           8260  0.059000           10.662228,\n",
       "           Phrase                              \n",
       "            count      freq avg_number_of_words\n",
       " Sentiment                                     \n",
       " 0            328  0.042051           13.435976\n",
       " 1           1519  0.194744            9.903226\n",
       " 2           3788  0.485641            5.714361\n",
       " 3           1696  0.217436            8.825472\n",
       " 4            469  0.060128           11.285714,\n",
       "           Phrase                              \n",
       "            count      freq avg_number_of_words\n",
       " Sentiment                                     \n",
       " 0            417  0.053462           12.177458\n",
       " 1           1605  0.205769            8.895327\n",
       " 2           3822  0.490000            5.639194\n",
       " 3           1522  0.195128            8.829172\n",
       " 4            434  0.055641           10.930876]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: get_by_phrase_stats(x), (train_data, valid_data, test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zdefiniowane zadania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W dalszych krokach będę próbował określić wartość kolumny `Sentiment` dla każdej z `Phrase`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Podejście z pełnymi zdaniami - ustalenie benchmarku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W tym podejściu skupiam się jedynie na pełnych zdaniach. Innymi słowy ignoruję podział zdania na frazy i jedynie na podstawie całego zdania próbuję określić wartość `Sentiment`. Dla fraz wchodzących w skład danego zdania przypiszę wartości `Sentiment` uzyskane dla tego zdania. Ze względu na niedoskonałość tego podejścia będę mógł je potraktować jako swego rodzaju benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W zestawie treningowym i walidacyjnym zostawiam tylko pełne zdania:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_full_sent_only = train_data.drop_duplicates(subset='SentenceId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>This quiet , introspective and entertaining in...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>82</td>\n",
       "      <td>3</td>\n",
       "      <td>Even fans of Ismail Merchant 's work , I suspe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>117</td>\n",
       "      <td>4</td>\n",
       "      <td>A positively thrilling combination of ethnogra...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>157</td>\n",
       "      <td>5</td>\n",
       "      <td>Aggressive self-glorification and a manipulati...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PhraseId  SentenceId                                             Phrase  \\\n",
       "0           1           1  A series of escapades demonstrating the adage ...   \n",
       "63         64           2  This quiet , introspective and entertaining in...   \n",
       "81         82           3  Even fans of Ismail Merchant 's work , I suspe...   \n",
       "116       117           4  A positively thrilling combination of ethnogra...   \n",
       "156       157           5  Aggressive self-glorification and a manipulati...   \n",
       "\n",
       "     Sentiment  \n",
       "0            1  \n",
       "63           4  \n",
       "81           1  \n",
       "116          3  \n",
       "156          1  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_full_sent_only.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data_full_sent_only = valid_data.drop_duplicates(subset='SentenceId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140461</td>\n",
       "      <td>7622</td>\n",
       "      <td>democracy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>140465</td>\n",
       "      <td>7623</td>\n",
       "      <td>Griffin &amp; Co. manage to be spectacularly outra...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>140475</td>\n",
       "      <td>7624</td>\n",
       "      <td>Like The English Patient and The Unbearable Li...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>140510</td>\n",
       "      <td>7625</td>\n",
       "      <td>`` Auto Focus '' works as an unusual biopic an...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>140529</td>\n",
       "      <td>7626</td>\n",
       "      <td>Very amusing , not the usual route in a thrill...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    PhraseId  SentenceId                                             Phrase  \\\n",
       "0     140461        7622                                          democracy   \n",
       "4     140465        7623  Griffin & Co. manage to be spectacularly outra...   \n",
       "14    140475        7624  Like The English Patient and The Unbearable Li...   \n",
       "49    140510        7625  `` Auto Focus '' works as an unusual biopic an...   \n",
       "68    140529        7626  Very amusing , not the usual route in a thrill...   \n",
       "\n",
       "    Sentiment  \n",
       "0           2  \n",
       "4           4  \n",
       "14          3  \n",
       "49          3  \n",
       "68          3  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data_full_sent_only.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniższa funkcja sprowadza dane zdanie do mniej złożonej reprezentacji (nadal literowej):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "def simplify_sentence(sentence):\n",
    "    # usuwanie znaków specjalnych (wszystkich poza alfanumerycznymi)\n",
    "    sentence = re.sub(r'\\W', ' ', sentence)\n",
    "    \n",
    "    # usuwanie wszystkich pojedynczych liter\n",
    "    sentence = re.sub('(\\\\b[A-Za-z] \\\\b|\\\\b [A-Za-z]\\\\b)', '', sentence)\n",
    "\n",
    "    # zamiana na małe litery\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # wyciągnięcie pojedynczych słów\n",
    "    sentence = sentence.split()\n",
    "    \n",
    "    # sprowadzenie słów do form podstawowych (lematów)\n",
    "    # uwaga: sentence to teraz lista\n",
    "    sentence = [stemmer.lemmatize(word) for word in sentence]\n",
    "    \n",
    "    # powrót do str\n",
    "    return \" \".join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pawel/.local/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "train_data_full_sent_only['Phrase_cleaned'] = train_data_full_sent_only['Phrase'].apply(lambda x: simplify_sentence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pawel/.local/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "valid_data_full_sent_only['Phrase_cleaned'] = valid_data_full_sent_only['Phrase'].apply(lambda x: simplify_sentence(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zamieniam uproszczone (wyczyszczone zdania) na reprezentację bag of words. \n",
    "\n",
    "`max_features`: zwracam uwagę tylko na 1000 najczęściej występujących słów, \n",
    "\n",
    "`min_df` to liczba minimalna liczba zdań, w których musi wystąpić słowo, żeby nie zostać zignorowanym, \n",
    "\n",
    "`max_df` to maksymalny odsetek wszystkich zdań, w których słowo może wystąpić - służy do zignorowania słów, które występują w prawie każdym zdaniu\n",
    "\n",
    "`stop_words` to słowa, które występują na tyle często w języku, że nie wnoszą żadnych informacji w zadaniu klasyfikacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=30, min_df=5, max_df=0.75, stop_words=stopwords.words('english'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_vectorized = vectorizer.fit_transform(train_data_full_sent_only['Phrase_cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data_vectorized = vectorizer.fit_transform(valid_data_full_sent_only['Phrase_cleaned'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dotychczasowa reprezentacja wektorowa zdań mimo że uwzględnia fakt bardzo rzadkich słów (`min_df`) oraz słów występujących w prawie każdym zdaniu (`max_df`) może zostać ulepszona podejściem TFIDF. Podejście to nadaje większą wagę słowom bardziej informatywnym, tzn. tym które występują często w małej ilości zdań."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfconverter = TfidfTransformer()  \n",
    "train_data_tfidf = tfidfconverter.fit_transform(train_data_vectorized).toarray() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data_tfidf = tfidfconverter.fit_transform(valid_data_vectorized).toarray() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Używam lasu losowego z 500 drzewami w celu dokonania klasyfikacji:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators=500, random_state=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_classifier.fit(train_data_tfidf, train_data_full_sent_only['Sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przygotowuję dane testowe, na podstawie których dokonam predykcji:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_full_sent_only = test_data.drop_duplicates(subset='SentenceId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pawel/.local/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "test_data_full_sent_only['Phrase_cleaned'] = test_data_full_sent_only['Phrase'].apply(lambda x: simplify_sentence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_vectorize = vectorizer.fit_transform(test_data_full_sent_only['Phrase_cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_tfidf = tfidfconverter.fit_transform(test_data_vectorize).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_classifier.predict(test_data_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`y_pred` muszą zostać skopiowane do odpowiadających `PhraseId`, aby można było obliczyć dokładność klasyfikatora. Z jednej strony mam tyle unikalnych `SentenceId` w `test_data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "475"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... z drugiej strony w `test_data` jest tyle unikalnych `PhraseId`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7800"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wykonuję złączenie dwóch ramek danych po `SentenceId`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_only_preds = pd.DataFrame(data={\"predicted_class\": y_pred, \"SentenceId\": test_data_full_sent_only['SentenceId']}, index=test_data_full_sent_only.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_class</th>\n",
       "      <th>SentenceId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>8068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>8069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>8070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>4</td>\n",
       "      <td>8071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1</td>\n",
       "      <td>8072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    predicted_class  SentenceId\n",
       "0                 3        8068\n",
       "5                 1        8069\n",
       "23                3        8070\n",
       "61                4        8071\n",
       "71                1        8072"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_only_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>predicted_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148261</td>\n",
       "      <td>8068</td>\n",
       "      <td>feminist conspiracy theorist</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>148262</td>\n",
       "      <td>8068</td>\n",
       "      <td>conspiracy theorist</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>148263</td>\n",
       "      <td>8068</td>\n",
       "      <td>theorist</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>148264</td>\n",
       "      <td>8068</td>\n",
       "      <td>named Dirty Dick</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>148265</td>\n",
       "      <td>8068</td>\n",
       "      <td>Dirty Dick</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                        Phrase  Sentiment  \\\n",
       "0    148261        8068  feminist conspiracy theorist          2   \n",
       "1    148262        8068           conspiracy theorist          2   \n",
       "2    148263        8068                      theorist          2   \n",
       "3    148264        8068              named Dirty Dick          2   \n",
       "4    148265        8068                    Dirty Dick          2   \n",
       "\n",
       "   predicted_class  \n",
       "0                3  \n",
       "1                3  \n",
       "2                3  \n",
       "3                3  \n",
       "4                3  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_w_predicted = pd.merge(test_data, sent_only_preds, how='left', on='SentenceId')\n",
    "test_data_w_predicted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biorąc pod uwagę tragiczne wyniki klasyfikatora za benchmark w dalszych rozważaniach będę uznawał częstość występowania klasy dominującej w zadaniach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20717948717948717\n",
      "Benchmark dla accuracy będzie wynosił:  0.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pawel/.local/lib/python3.6/site-packages/pandas/core/groupby/groupby.py:4656: FutureWarning: using a dict with renaming is deprecated and will be removed in a future version\n",
      "  return super(DataFrameGroupBy, self).aggregate(arg, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(test_data_w_predicted['Sentiment'], test_data_w_predicted['predicted_class']))\n",
    "print(\"Benchmark dla accuracy będzie wynosił: \", max(get_by_phrase_stats(test_data)['Phrase']['freq']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelowanie embeddingsów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W dalszej części analizy będę posługiwał się podejściem z uczenia głębokiego. W przypadku modelowania tekstu potrzebne jest utworzenie wektorów zanurzonych. Na początku spróbuję utworzyć je własnoręcznie, później skorzystam z już obliczonych, publicznie dostępnych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeggings - 24.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Czyszczenie danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniżej zdefiniowana funkcja jest zbyt agresywna, ponieważ dla niektórych obserwacji w `Phrase` (np. tych zawierających same *stopwords*) będzie zwracała puste napisy. W jej miejsce stosuję wyrażenie regularne, które usuwa wszystkie znaki poza literami łacińskimi (małymi i wielkimi), cyframi oraz odstępami (np. spacje, taby, nowe linie). Mimo, że wykorzystany w dalszej części obiekt `Tokenizer` filtruje po znakach m.in. znakach odstępu (`\\s`) usuwając je, to na razie muszę zostawić odstępy, które stanowią granicę wyrazów (tokenów) w zdaniu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Phrase'] = train_data['Phrase'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['Phrase'] = test_data['Phrase'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148261</td>\n",
       "      <td>8068</td>\n",
       "      <td>feminist conspiracy theorist</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>148262</td>\n",
       "      <td>8068</td>\n",
       "      <td>conspiracy theorist</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>148263</td>\n",
       "      <td>8068</td>\n",
       "      <td>theorist</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>148264</td>\n",
       "      <td>8068</td>\n",
       "      <td>named Dirty Dick</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>148265</td>\n",
       "      <td>8068</td>\n",
       "      <td>Dirty Dick</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                        Phrase  Sentiment\n",
       "0    148261        8068  feminist conspiracy theorist          2\n",
       "1    148262        8068           conspiracy theorist          2\n",
       "2    148263        8068                      theorist          2\n",
       "3    148264        8068              named Dirty Dick          2\n",
       "4    148265        8068                    Dirty Dick          2"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na wejściu do sieci musi pojawiać się wektor stałej długości. Dodatkowym wymaganiem jest to, żeby słowa w nim zawarte były zakodowane jako liczby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = train_data['Phrase']\n",
    "y_tr = train_data['Sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiuję `Tokenizer`, którego jednym z argumentów jest m.in. to ile słów z wejścia zachowa. Domyślne ustawienia (`num_words`) to `num_words-1`, co oznacza, że słowa o najniższej częstości występowania nie przejdą etapu tokenizacji. Wydaje się to rozsądnym pomysłem, ponieważ słowa najrzadziej występujące to zazwyczaj te występujące tylko raz. Dodatkowo przerabiam wszystkie znaki na wersje pisane małą literą (argument `lower=True`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W tym kroku tworzę słownik, który ma następującą postać `słownik['the']=1`, czyli każde słowo z zestawu treningowego jest kodowane jako liczba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(X_tr.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ts = test_data['Phrase']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniżej elementy zdania zamieniane są na liczby:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_tokenized = tokenizer.texts_to_sequences(X_tr)\n",
    "X_ts_tokenized = tokenizer.texts_to_sequences(X_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniższy przykład pokazuje, że zdania zostały zamienione na listy długości równej liczbie tokenów w pierwotnych (ale oczyszczonych regexem) zdaniach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('to much of a story', [5, 54, 3, 2, 40])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr[55], X_tr_tokenized[55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('for excitement', [13, 1472])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ts[22], X_ts_tokenized[22]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Należy wyrównać listy do tej samej długości (długości najliczniejszego pod względem tokenów zdania). Argument `padding='post'` wskazuje, że w krótszych niż `max_len` zdaniach zera będą dodane na końcu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = max([len(x.split()) for x in train_data['Phrase']])\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_tokenized = pad_sequences(X_tr_tokenized, max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   2,  316,    3, ...,    0,    0,    0],\n",
       "       [   2,  316,    3, ...,    0,    0,    0],\n",
       "       [   2,  316,    0, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 280,  442, 3148, ...,    0,    0,    0],\n",
       "       [   8,   28,  482, ...,    0,    0,    0],\n",
       "       [  28,  482,   17, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ts_tokenized = pad_sequences(X_ts_tokenized, max_len, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Długość tokenizowanych zdań w obu zestawach jest równa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_tokenized.shape[1] == X_ts_tokenized.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Budowa sieci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Każdy z wektorów zanurzonych w sieci będzie miał wymiar 100. Zgodnie z dokumentacją `input_dim` dla w `Embedding()` musi być równy liczbie słów plus 1 (plus 1, aby ująć `0` za pomocą, którego wypełnia się wektory krótsze niż maksymalny wymiar). `input_length` to argument informujący o długości wejścia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dim = 300\n",
    "no_epochs = 50\n",
    "input_dim = len(tokenizer.word_index)+1\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=input_dim, output_dim=embeddings_dim, input_length=max_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warstwa Long Short Term Memory (`LSTM`) oraz warstawa klasyfikująca `Dense`, w której umieszczam 5 neuronów (jedna dla każdej z przewidywanych klas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0524 22:51:58.606062 140284932192064 deprecation.py:506] From /home/pawel/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:4081: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model.add(LSTM(units=128, dropout=0.33, recurrent_dropout=0.33))\n",
    "# model.add(LSTM(units=64, dropout=0.2, recurrent_dropout=0.2, return_sequences=False))\n",
    "# model.add(Dense(100, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(units=5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warto zwrócić uwagę na funkcję straty `'sparse_categorical_crossentropy'`, która powinna być używana w przypadku, gdy klasy są rozłączne. W naszym przypadku oznacza to, że ostatnia warstwa nie zwraca prawdopodobieństw należenia do różnych klas. Wspomniana funkcja jest używana, gdy zmienna zależna nie jest kodowana w postaci one-hot, lecz jako liczby całkowite (np. 1, 2, 3, 4, 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['mse', 'acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 48, 300)           4621800   \n",
      "_________________________________________________________________\n",
      "unified_lstm (UnifiedLSTM)   (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 4,842,093\n",
      "Trainable params: 4,842,093\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W tym miejscu należy zdefiniować dane walidacyjne, które należy przekazać w argumencie `validation_data`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = valid_data['Phrase']\n",
    "y_val = valid_data['Sentiment']\n",
    "\n",
    "X_val_tokenized = tokenizer.texts_to_sequences(X_val)\n",
    "X_val_tokenized = pad_sequences(X_val_tokenized, max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7800, 48)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_tokenized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aby zaradzić wcześniej wspomnianemu problemowi niezbalansowanych danych treningowych będę odpowiednio ważył obserwację z każdej kategorii. Kategoria najbardziej liczbna (`2`) ma teraz najmniejszą wagę"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 4.431750553972776,\n",
       " 1: 1.1627341057265064,\n",
       " 2: 0.3903063927067942,\n",
       " 3: 0.9458752786973853,\n",
       " 4: 3.3898062953995156}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_tr),\n",
    "                                                 y_tr)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podczas treningu będę także stosował mechanizm early stopping, który automatycznie zatrzyma trening jeżeli wcześniej określone kryterium nie zostanie spełnione. Proces treningu zostanie zatrzymany, jeżeli model w 5 epokach nie wykaże się wzrostem `accuracy` na zestawie walidacyjnym na poziomie co najmniej 0.003. Korzystam z `min_delta`, aby wykluczyć początkowe etapy treningu, gdzie mogą występować znaczące fluktuacje pomiędzy epokami. Innymi słowy ustalone zasady zaczynają obowiązywać dopiero po przekroczeniu `val_acc` równego 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(min_delta=0.001, \n",
    "                               monitor='val_acc', \n",
    "                               patience=5, \n",
    "                               mode='max',\n",
    "                               verbose=1,\n",
    "                               baseline=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stosuję także podejście pozwalające zapisać wersję modelu z treningu, która cechowała najlepszą wskazaną cechą. Tutaj skupię się na funkcji straty dla zestawu walidacyjnego (która powinna być minimalizowana):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = ModelCheckpoint('./models/model_val_loss_best.h5', \n",
    "                     monitor='val_loss', \n",
    "                     mode='min', \n",
    "                     save_best_only=True,\n",
    "                     verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = [early_stopping, mc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 139999 samples, validate on 7800 samples\n",
      "Epoch 1/50\n",
      "139968/139999 [============================>.] - ETA: 0s - loss: 1.5888 - mse: 4.2842 - acc: 0.2744\n",
      "Epoch 00001: val_loss improved from inf to 1.59119, saving model to ./models/model_val_loss_best.h5\n",
      "139999/139999 [==============================] - 248s 2ms/sample - loss: 1.5888 - mse: 4.2843 - acc: 0.2744 - val_loss: 1.5912 - val_mse: 4.2776 - val_acc: 0.2586\n",
      "Epoch 2/50\n",
      "139968/139999 [============================>.] - ETA: 0s - loss: 1.5445 - mse: 4.2879 - acc: 0.3633\n",
      "Epoch 00002: val_loss improved from 1.59119 to 1.43523, saving model to ./models/model_val_loss_best.h5\n",
      "139999/139999 [==============================] - 249s 2ms/sample - loss: 1.5445 - mse: 4.2877 - acc: 0.3633 - val_loss: 1.4352 - val_mse: 4.2840 - val_acc: 0.4432\n",
      "Epoch 3/50\n",
      "139968/139999 [============================>.] - ETA: 0s - loss: 1.2728 - mse: 4.3089 - acc: 0.5058\n",
      "Epoch 00003: val_loss improved from 1.43523 to 1.18483, saving model to ./models/model_val_loss_best.h5\n",
      "139999/139999 [==============================] - 254s 2ms/sample - loss: 1.2728 - mse: 4.3089 - acc: 0.5058 - val_loss: 1.1848 - val_mse: 4.3194 - val_acc: 0.5306\n",
      "Epoch 4/50\n",
      "139968/139999 [============================>.] - ETA: 0s - loss: 0.8729 - mse: 4.3406 - acc: 0.6140\n",
      "Epoch 00004: val_loss did not improve from 1.18483\n",
      "139999/139999 [==============================] - 260s 2ms/sample - loss: 0.8728 - mse: 4.3405 - acc: 0.6140 - val_loss: 1.2302 - val_mse: 4.3296 - val_acc: 0.5100\n",
      "Epoch 5/50\n",
      "139968/139999 [============================>.] - ETA: 0s - loss: 0.7652 - mse: 4.3517 - acc: 0.6440\n",
      "Epoch 00005: val_loss did not improve from 1.18483\n",
      "139999/139999 [==============================] - 264s 2ms/sample - loss: 0.7652 - mse: 4.3517 - acc: 0.6440 - val_loss: 1.1885 - val_mse: 4.3381 - val_acc: 0.5273\n",
      "Epoch 6/50\n",
      "139968/139999 [============================>.] - ETA: 0s - loss: 0.7040 - mse: 4.3591 - acc: 0.6591\n",
      "Epoch 00006: val_loss did not improve from 1.18483\n",
      "139999/139999 [==============================] - 268s 2ms/sample - loss: 0.7040 - mse: 4.3592 - acc: 0.6591 - val_loss: 1.2166 - val_mse: 4.3397 - val_acc: 0.5287\n",
      "Epoch 7/50\n",
      "139968/139999 [============================>.] - ETA: 0s - loss: 0.6573 - mse: 4.3648 - acc: 0.6733\n",
      "Epoch 00007: val_loss did not improve from 1.18483\n",
      "139999/139999 [==============================] - 272s 2ms/sample - loss: 0.6572 - mse: 4.3649 - acc: 0.6733 - val_loss: 1.2564 - val_mse: 4.3437 - val_acc: 0.5245\n",
      "Epoch 8/50\n",
      "139968/139999 [============================>.] - ETA: 0s - loss: 0.6199 - mse: 4.3695 - acc: 0.6827\n",
      "Epoch 00008: val_loss did not improve from 1.18483\n",
      "139999/139999 [==============================] - 274s 2ms/sample - loss: 0.6198 - mse: 4.3694 - acc: 0.6827 - val_loss: 1.2952 - val_mse: 4.3501 - val_acc: 0.5190\n",
      "Epoch 00008: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f962da10668>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_tr_tokenized, \n",
    "          y=y_tr, \n",
    "          epochs=no_epochs, \n",
    "          verbose=1,\n",
    "          batch_size=64,\n",
    "          validation_data=(X_val_tokenized, y_val),\n",
    "          class_weight=class_weights,\n",
    "          callbacks=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do ostatecznego sprawdzenia jakości modelu będą potrzebna zmienna zależna z zestawu testowego:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_data['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7800/7800 [==============================] - 3s 328us/sample - loss: 1.2992 - mse: 4.1307 - acc: 0.5231\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2991876481129574, 4.1307063, 0.52307695]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=X_ts_tokenized, y=y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
